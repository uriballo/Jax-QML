{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcb848c8cf6de1d",
   "metadata": {},
   "source": [
    "remember to create a virtual environment and install the requirements:\n",
    "\n",
    "```\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r reqs.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433be26c31e046d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91201a4abad6b98",
   "metadata": {},
   "source": [
    "# Introduction to JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5974f44de59b1",
   "metadata": {},
   "source": [
    "In short, JAX is an array-oriented numerical computing library that enables composable transformations. These include just-in-time (JIT) compilation, automatic vectorization and automatic differentiation. On top of that, since it leverages the XLA (Accelerated Linear Algebra)\n",
    "compiler it can run on CPUs, GPUs and TPUs natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3538336c7ead3665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:47:06.702823Z",
     "start_time": "2025-02-11T10:47:06.436850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0)]\n",
      "Result on CPU: [[12. 12. 12.]\n",
      " [12. 12. 12.]\n",
      " [12. 12. 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "\n",
    "def compute(x):\n",
    "    return jnp.dot(x, x.T) + jnp.sum(x)\n",
    "\n",
    "x = jnp.ones((3, 3))\n",
    "\n",
    "# Run on CPU\n",
    "cpu_result = jax.jit(compute, backend=\"cpu\")(x)\n",
    "print(\"Result on CPU:\", cpu_result)\n",
    "\n",
    "# Run on GPU (if available)\n",
    "if any(d.device_kind == \"GPU\" for d in jax.devices()):\n",
    "    gpu_result = jax.jit(compute, backend=\"gpu\")(x)\n",
    "    print(\"Result on GPU:\", gpu_result)\n",
    "\n",
    "# Run on TPU (if available)\n",
    "if any(d.device_kind == \"TPU\" for d in jax.devices()):\n",
    "    tpu_result = jax.jit(compute, backend=\"tpu\")(x)\n",
    "    print(\"Result on TPU:\", tpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe4ffe587e8bf0",
   "metadata": {},
   "source": [
    "JAX provides an array interface that mimics NumPy and can be used as a drop-in replacement for NumPy arrays. The most notable difference between \\texttt{numpy}, usually referred as \\texttt{np}, arrays and \\texttt{jax.numpy}, usually referred as \\texttt{jnp}, is that the latter are always immutable.\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbd41dbb85a3ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:47:42.082936Z",
     "start_time": "2025-02-11T10:47:42.078687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NumPy array: [1 2 3]\n",
      "Modified NumPy array: [10  2  3]\n",
      "\n",
      "[!] JAX array mutation error: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n",
      "\n",
      "Original JAX array: [1 2 3]\n",
      "New JAX array after modification: [10  2  3]\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "np_array = np.array([1, 2, 3])\n",
    "print(\"Original NumPy array:\", np_array)\n",
    "np_array[0] = 10  # Mutating a NumPy array is allowed\n",
    "print(\"Modified NumPy array:\", np_array)\n",
    "\n",
    "# Create a JAX array\n",
    "jnp_array = jnp.array([1, 2, 3])\n",
    "\n",
    "try:\n",
    "    jnp_array[0] = 10  # Attempting to mutate a JAX array\n",
    "except TypeError as e:\n",
    "    print(\"\\n[!] JAX array mutation error:\", e)\n",
    "\n",
    "# Instead of mutating, we create a new array\n",
    "new_jnp_array = jnp_array.at[0].set(10)\n",
    "print(\"\\nOriginal JAX array:\", jnp_array)\n",
    "print(\"New JAX array after modification:\", new_jnp_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fd6b50507d5d3",
   "metadata": {},
   "source": [
    "All JAX operations are implemented in terms of XLA. This means that whenever we use \\texttt{jnp} we will be taking advantage of the accelerated linear algebra compiler. However, we also have a lower level API available with \\texttt{jax.lax}, which contains wrappers for primitive XLA operations. All \\texttt{jnp} operations are implemented in terms of \\texttt{jax.lax}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5a35f",
   "metadata": {},
   "source": [
    "JAX has an internal representation of programs called jaxpr language. We can use the \\texttt{jax.make\\_jaxpr} function to obtain the jaxpr representation of a function. This can be useful for finding out how certain functions get transformed to lower level operations. For instance, we can see how \\texttt{jnp.dot} translates to a more general \\texttt{jax.lax.dot\\_general}. \\newpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d5b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAXpr for jnp.dot:\n",
      " { lambda ; a:i32[2,2] b:i32[2,2]. let\n",
      "    c:i32[2,2] = dot_general[\n",
      "      dimension_numbers=(([1], [0]), ([], []))\n",
      "      preferred_element_type=int32\n",
      "    ] a b\n",
      "  in (c,) } \n",
      "\n",
      "JAXpr for jax.lax.dot_general:\n",
      " { lambda ; a:i32[2,2] b:i32[2,2]. let\n",
      "    c:i32[2,2] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n",
      "  in (c,) }\n"
     ]
    }
   ],
   "source": [
    "def high_level(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "def low_level(x, y):\n",
    "    return jax.lax.dot_general(\n",
    "        x, y,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ()))  # Batch dimensions are empty\n",
    "    )\n",
    "    \n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "jaxpr_high_level = jax.make_jaxpr(high_level)(A, B)\n",
    "jaxpr_low_level = jax.make_jaxpr(low_level)(A, B)\n",
    "\n",
    "print(\"JAXpr for jnp.dot:\\n\", jaxpr_high_level, \"\\n\")\n",
    "print(\"JAXpr for jax.lax.dot_general:\\n\", jaxpr_low_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43014841ade47613",
   "metadata": {},
   "source": [
    "Along with a NumPy-like API of functions that operate on arrays, JAX also includes a number of composable transformations which operate on functions. The ones we are interested the most in are:\n",
    "- `jax.jit`.\n",
    "- `jax.vmap`.\n",
    "- `jax.grad`.\n",
    "\n",
    "To do transformations, JAX uses the concept of tracing a function. Tracing works by replacing the array inputs of a function by abstract placeholders with the same shape and type. This allows JAX to determine the sequence of operations of a function and the effect these have on the input arrays, independently of their content.\n",
    "\n",
    "We can see how JAX sees traced arrays by printing inside a function subject to a transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67eac0fd0404125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T12:03:11.960114Z",
     "start_time": "2025-02-11T12:03:11.919740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outside the function we see x as  [0 1 2 3 4]\n",
      "inside the function we see x as  Traced<ShapedArray(int32[5])>with<DynamicJaxprTrace>\n",
      "f(x) =  [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "  print(\"inside the function we see x as \", x)\n",
    "  return x + 1\n",
    "\n",
    "x = jnp.arange(5)\n",
    "print(\"outside the function we see x as \", x)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73849719e8885",
   "metadata": {},
   "source": [
    "The printed value we see inside the function is not the array $x$, but rather an abstract traced representation that has the same shape and type. In this case, the JIT transform has used the traced information to create a compiled version of $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650780edd9f2aba",
   "metadata": {},
   "source": [
    "## Just-in-Time Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bfbc682c30248",
   "metadata": {},
   "source": [
    "By default, JAX executes operations eagerly, dispatching each operation individually to XLA without ahead-of-time compilation. The \\texttt{jax.jit} transform leverages JAXâ€™s tracing mechanism to capture the flow of entire functions, allowing the XLA compiler to optimize, fuse, and compile sequences of operations into a single efficient execution. \n",
    "\n",
    "When using JIT, we exchange a slow first execution for much faster subsequent ones. Therefore, JIT is the most useful when we have expensive numerical functions that we need to run a bunch of times.\n",
    "\n",
    "Despite being very powerful, JIT compilation still has some limitations. In particular, due the tracing mechanism, it requires that branching operations do not depend on the contents of traced parameters. This implies that the following functions will not work with JIT.\n",
    "\n",
    "In this case because the value of $n$ will not be known at trace-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464da6d077fd5d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.666854Z",
     "start_time": "2025-02-11T09:17:49.659308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] JAX JIT error: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function g at /var/folders/6q/mzgmvrhn3l76p312t6zn3mxw0000gn/T/ipykernel_8507/870922876.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\n"
     ]
    }
   ],
   "source": [
    "def g(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "try:\n",
    "    jax.jit(g)(10, 20)  # Raises an error\n",
    "except Exception as e:\n",
    "    print(\"[!] JAX JIT error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaac6ee",
   "metadata": {},
   "source": [
    "In this case because the final shape of the output will not be known at trace-time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "debd2ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] JAX JIT error: Array boolean indices must be concrete; got ShapedArray(bool[10])\n",
      "\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError\n"
     ]
    }
   ],
   "source": [
    "def get_negatives(x):\n",
    "  return x[x < 0]\n",
    "\n",
    "x = jnp.array(np.random.randn(10))\n",
    "\n",
    "try:\n",
    "    jax.jit(get_negatives)(x)  # Raises an error\n",
    "except Exception as e:\n",
    "    print(\"[!] JAX JIT error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f55cb3",
   "metadata": {},
   "source": [
    "Another way to see this limitation is that branching statements are only allowed in JIT functions if and only if they are based on static attributes, like shape and type.\n",
    "\n",
    "A simple solution to deal with functions like the previous ones is to separate the statement that comes after the branching in an external function and compile that rather than the whole function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42aff5f661763c60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.715318Z",
     "start_time": "2025-02-11T09:17:49.674158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# While loop conditioned on x and n with a jitted body.\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i = loop_body(i)\n",
    "  return x + i\n",
    "\n",
    "x = g_inner_jitted(10, 20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0518ef",
   "metadata": {},
   "source": [
    "Alternatively, if we really need to compile the whole function we can mark certain arguments as static. This will make the compiled version of the function depend on the static arguments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c129a49b2963e4a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.738184Z",
     "start_time": "2025-02-11T09:17:49.720717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['n'])\n",
    "def g_jit_decorated(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "x = g_jit_decorated(10, 20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8517e",
   "metadata": {},
   "source": [
    "This will make it so JAX has to re-compile the function for every new static input. Therefore, it is only a good strategy if we know that there is only a limited number cases.\n",
    "\n",
    "Subsequent calls of JIT compiled functions use the cached code, which they find via the hash of the function. This implies that we have to be a bit careful when applying the JIT transformation. In some cases, by mistake, we may redefine an equivalent function in a way that modifies its hash and makes it impossible for JIT to use the cached compiled code. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95747f53fcad795c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.745870Z",
     "start_time": "2025-02-11T09:17:49.743643Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unjitted_loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted_partial(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this! each time the partial returns\n",
    "    # a function with different hash\n",
    "    i = jax.jit(partial(unjitted_loop_body))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_lambda(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this!, lambda will also return\n",
    "    # a function with a different hash\n",
    "    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_normal(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # this is OK, since JAX can find the\n",
    "    # cached, compiled function\n",
    "    i = jax.jit(unjitted_loop_body)(i)\n",
    "  return x + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fa5488d0a0897f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:57.896076Z",
     "start_time": "2025-02-11T09:17:49.754314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 ms Â± 8.85 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "236 ms Â± 4.17 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "603 Î¼s Â± 4.95 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
    "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n",
    "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46baa308",
   "metadata": {},
   "source": [
    "As we can see in the time per loop, the first two are much slower due the fact of not being able to take advantage of the cached code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acdf6a26d0fb70",
   "metadata": {},
   "source": [
    "## Automatic Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c99956d416a0e",
   "metadata": {},
   "source": [
    "The automatic vectorization transform allows us to extend a function defined on single inputs to support batch operations, while leveraging hardware acceleration whenever possible.\n",
    "Automatic vectorization allows us to extend a function defined on single input to support batch operations, while capitalizing on hardware acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e7521",
   "metadata": {},
   "source": [
    "To use the \\texttt{jax.vmap} we just have to specify how to vectorize the function with the \\texttt{in\\_axes} and \\texttt{out\\_axes} parameters.\n",
    "\n",
    "The \\texttt{in\\_axes} parameter specifies along which axes we vectorize each input. In this case we specify to vectorize along the rows, $0$, for the first two parameters and to not vectorize along the third parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe33802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.7501   , 2.3000998, 1.8001001], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_array_sum(scale: float, weights: jnp.ndarray, epsilon: float):\n",
    "    return scale * jnp.sum(weights) + epsilon\n",
    "    \n",
    "scales = jnp.array([0.5, 1.0, 1.5]) \n",
    "weights = jnp.array([\n",
    "    [1.0, 0.2, 0.3],\n",
    "    [0.5, 0.8, 1.0],\n",
    "    [0.7, 0.1, 0.4]\n",
    "])\n",
    "epsilon = 1e-4\n",
    "\n",
    "vectorized_weighted_array_sum = jax.vmap(weighted_array_sum, in_axes=(0, 0, None))\n",
    "\n",
    "vectorized_weighted_array_sum(scales, weights, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b0f61",
   "metadata": {},
   "source": [
    "The \\texttt{out\\_axes} parameter specifies how to store the batched results. In this case we show the difference of storing the batched results as rows or as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41d6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_rows(x) = \n",
      " [[1.  0.2 0.3]\n",
      " [0.5 0.8 1. ]\n",
      " [0.7 0.1 0.4]] \n",
      "\n",
      "id_cols(x) = \n",
      " [[1.  0.5 0.7]\n",
      " [0.2 0.8 0.1]\n",
      " [0.3 1.  0.4]]\n"
     ]
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "print(\"id_rows(x) = \\n\", jax.vmap(identity, in_axes=0, out_axes=0)(weights), \"\\n\")\n",
    "print(\"id_cols(x) = \\n\", jax.vmap(identity, in_axes=0, out_axes=1)(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5312a",
   "metadata": {},
   "source": [
    "All JAX transformations are composable, this means that we can combine \\texttt{jax.jit} and \\texttt{jax.vmap} to create just-in-time compiled-vectorized  functions. It's important to note that these two transformations commute, it doesn't matter if we compile or vectorize first, it will always create a compiled version of the vectorized function rather than a vectorized version of compiled functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab1865bcd04922",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8dfba6d8aa900",
   "metadata": {},
   "source": [
    "The automatic differentiation transform is a pretty simple transform that allows us to get the gradient of any function. We can apply it multiple times to obtain a higher order gradient. Also we can choose along which parameters to compute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "552872419e1c37f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:17:02.427646Z",
     "start_time": "2025-02-11T10:17:02.419367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t x: 4.0\n",
      "Gradient w.r.t y: 27.0\n",
      "Gradient both: (Array(4., dtype=float32, weak_type=True), Array(27., dtype=float32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "def func(x, y):\n",
    "    return x**2 + y**3\n",
    "\n",
    "grad_x = jax.grad(func, argnums=0)        # Derivative w.r.t. x\n",
    "grad_y = jax.grad(func, argnums=1)        # Derivative w.r.t. y\n",
    "grad   = jax.grad(func, argnums= (0, 1))  # Both\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "print(\"Gradient w.r.t x:\", grad_x(x, y))  # 2*x = 4\n",
    "print(\"Gradient w.r.t y:\", grad_y(x, y))  # 3*y^2 = 27\n",
    "print(\"Gradient both:\", grad(x, y))       # (4, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "386c6e924edb43a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:19:18.825592Z",
     "start_time": "2025-02-11T10:19:18.730500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: {'b': Array(30., dtype=float32, weak_type=True), 'w': Array(68., dtype=float32, weak_type=True)}\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(params, x):\n",
    "    w, b = params[\"w\"], params[\"b\"]\n",
    "    return jnp.sum((w * x + b) ** 2)\n",
    "\n",
    "params = {\"w\": 2.0, \"b\": 1.0}\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grads = grad_fn(params, x)\n",
    "print(\"Gradients:\", grads)  # {'w': ..., 'b': ...}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
