{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcb848c8cf6de1d",
   "metadata": {},
   "source": [
    "remember to create a virtual environment and install the requirements:\n",
    "\n",
    "```\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r reqs.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433be26c31e046d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91201a4abad6b98",
   "metadata": {},
   "source": [
    "# Introduction to JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5974f44de59b1",
   "metadata": {},
   "source": [
    "In short, JAX is an array-oriented numerical computing library that enables composable transformations. These include just-in-time (JIT) compilation, automatic vectorization and automatic differentiation. On top of that, since it leverages the XLA (Accelerated Linear Algebra)\n",
    "compiler it can run on CPUs, GPUs and TPUs natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3538336c7ead3665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:47:06.702823Z",
     "start_time": "2025-02-11T10:47:06.436850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0)]\n",
      "Result on CPU: [[12. 12. 12.]\n",
      " [12. 12. 12.]\n",
      " [12. 12. 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "\n",
    "def compute(x):\n",
    "    return jnp.dot(x, x.T) + jnp.sum(x)\n",
    "\n",
    "x = jnp.ones((3, 3))\n",
    "\n",
    "# Run on CPU\n",
    "cpu_result = jax.jit(compute, backend=\"cpu\")(x)\n",
    "print(\"Result on CPU:\", cpu_result)\n",
    "\n",
    "# Run on GPU (if available)\n",
    "if any(d.device_kind == \"GPU\" for d in jax.devices()):\n",
    "    gpu_result = jax.jit(compute, backend=\"gpu\")(x)\n",
    "    print(\"Result on GPU:\", gpu_result)\n",
    "\n",
    "# Run on TPU (if available)\n",
    "if any(d.device_kind == \"TPU\" for d in jax.devices()):\n",
    "    tpu_result = jax.jit(compute, backend=\"tpu\")(x)\n",
    "    print(\"Result on TPU:\", tpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe4ffe587e8bf0",
   "metadata": {},
   "source": [
    "JAX provides an array interface that mimics NumPy and can be used as a drop-in replacement for NumPy arrays. The most notable difference between \\texttt{numpy}, usually referred as \\texttt{np}, arrays and \\texttt{jax.numpy}, usually referred as \\texttt{jnp}, is that the latter are always immutable.\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbd41dbb85a3ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:47:42.082936Z",
     "start_time": "2025-02-11T10:47:42.078687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NumPy array: [1 2 3]\n",
      "Modified NumPy array: [10  2  3]\n",
      "\n",
      "[!] JAX array mutation error: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n",
      "\n",
      "Original JAX array: [1 2 3]\n",
      "New JAX array after modification: [10  2  3]\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "np_array = np.array([1, 2, 3])\n",
    "print(\"Original NumPy array:\", np_array)\n",
    "np_array[0] = 10  # Mutating a NumPy array is allowed\n",
    "print(\"Modified NumPy array:\", np_array)\n",
    "\n",
    "# Create a JAX array\n",
    "jnp_array = jnp.array([1, 2, 3])\n",
    "\n",
    "try:\n",
    "    jnp_array[0] = 10  # Attempting to mutate a JAX array\n",
    "except TypeError as e:\n",
    "    print(\"\\n[!] JAX array mutation error:\", e)\n",
    "\n",
    "# Instead of mutating, we create a new array\n",
    "new_jnp_array = jnp_array.at[0].set(10)\n",
    "print(\"\\nOriginal JAX array:\", jnp_array)\n",
    "print(\"New JAX array after modification:\", new_jnp_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fd6b50507d5d3",
   "metadata": {},
   "source": [
    "All JAX operations are implemented in terms of XLA. This means that whenever we use \\texttt{jnp} we will be taking advantage of the accelerated linear algebra compiler. However, we also have a lower level API available with \\texttt{jax.lax}, which contains wrappers for primitive XLA operations. All \\texttt{jnp} operations are implemented in terms of \\texttt{jax.lax}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5a35f",
   "metadata": {},
   "source": [
    "JAX has an internal representation of programs called jaxpr language. We can use the \\texttt{jax.make\\_jaxpr} function to obtain the jaxpr representation of a function. This can be useful for finding out how certain functions get transformed to lower level operations. For instance, we can see how \\texttt{jnp.dot} translates to a more general \\texttt{jax.lax.dot\\_general}. \\newpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d5b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAXpr for jnp.dot:\n",
      " { lambda ; a:i32[2,2] b:i32[2,2]. let\n",
      "    c:i32[2,2] = dot_general[\n",
      "      dimension_numbers=(([1], [0]), ([], []))\n",
      "      preferred_element_type=int32\n",
      "    ] a b\n",
      "  in (c,) } \n",
      "\n",
      "JAXpr for jax.lax.dot_general:\n",
      " { lambda ; a:i32[2,2] b:i32[2,2]. let\n",
      "    c:i32[2,2] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n",
      "  in (c,) }\n"
     ]
    }
   ],
   "source": [
    "def high_level(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "def low_level(x, y):\n",
    "    return jax.lax.dot_general(\n",
    "        x, y,\n",
    "        dimension_numbers=(((1,), (0,)), ((), ()))  # Batch dimensions are empty\n",
    "    )\n",
    "    \n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "jaxpr_high_level = jax.make_jaxpr(high_level)(A, B)\n",
    "jaxpr_low_level = jax.make_jaxpr(low_level)(A, B)\n",
    "\n",
    "print(\"JAXpr for jnp.dot:\\n\", jaxpr_high_level, \"\\n\")\n",
    "print(\"JAXpr for jax.lax.dot_general:\\n\", jaxpr_low_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43014841ade47613",
   "metadata": {},
   "source": [
    "Along with a NumPy-like API of functions that operate on arrays, JAX also includes a number of composable transformations which operate on functions. The ones we are interested the most in are:\n",
    "- `jax.jit`.\n",
    "- `jax.vmap`.\n",
    "- `jax.grad`.\n",
    "\n",
    "To do transformations, JAX uses the concept of tracing a function. Tracing works by replacing the array inputs of a function by abstract placeholders with the same shape and type. This allows JAX to determine the sequence of operations of a function and the effect these have on the input arrays, independently of their content.\n",
    "\n",
    "We can see how JAX sees traced arrays by printing inside a function subject to a transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67eac0fd0404125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T12:03:11.960114Z",
     "start_time": "2025-02-11T12:03:11.919740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outside the function we see x as  [0 1 2 3 4]\n",
      "inside the function we see x as  Traced<ShapedArray(int32[5])>with<DynamicJaxprTrace>\n",
      "f(x) =  [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "  print(\"inside the function we see x as \", x)\n",
    "  return x + 1\n",
    "\n",
    "x = jnp.arange(5)\n",
    "print(\"outside the function we see x as \", x)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73849719e8885",
   "metadata": {},
   "source": [
    "The printed value we see inside the function is not the array $x$, but rather an abstract traced representation that has the same shape and type. In this case, the JIT transform has used the traced information to create a compiled version of $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650780edd9f2aba",
   "metadata": {},
   "source": [
    "## Just-in-Time Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bfbc682c30248",
   "metadata": {},
   "source": [
    "By default, JAX executes operations eagerly, dispatching each operation individually to XLA without ahead-of-time compilation. The \\texttt{jax.jit} transform leverages JAXâ€™s tracing mechanism to capture entire computations, allowing the XLA compiler to optimize, fuse, and compile sequences of operations into a single efficient execution.\n",
    "\n",
    "JIT compilation is very powerful but it has some limitations. In particular, it requires that branching operations must be determined and trace-time. This implies that the following function will not work with JIT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464da6d077fd5d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.666854Z",
     "start_time": "2025-02-11T09:17:49.659308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[!] JAX JIT error: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function g at /var/folders/6q/mzgmvrhn3l76p312t6zn3mxw0000gn/T/ipykernel_2089/1615799189.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\n"
     ]
    }
   ],
   "source": [
    "def g(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "try:\n",
    "    jax.jit(g)(10, 20)  # Raises an error\n",
    "except TypeError as e:\n",
    "    print(\"\\t[!] JAX JIT error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31acfad1f1f1b5",
   "metadata": {},
   "source": [
    "The reason of these errors is that branching statements are allowed in JIT functions as long as they are based on static attributes, e.g. shape, type... since they can be determined at trace-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43c2a06343ee6f",
   "metadata": {},
   "source": [
    "What we can do to deal with this is to only compile  certain parts of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42aff5f661763c60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.715318Z",
     "start_time": "2025-02-11T09:17:49.674158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# While loop conditioned on x and n with a jitted body.\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i = loop_body(i)\n",
    "  return x + i\n",
    "\n",
    "x = g_inner_jitted(10, 20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c7138081fe459",
   "metadata": {},
   "source": [
    "If we really need to compile the whole function then we can mark certain arguments as static. This will make the compiled version of the function depend on the static arguments, so JAX will have to re-compile the function for every new static input. This is only a good strategy if the you know that there is only a limited number of static values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c129a49b2963e4a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.738184Z",
     "start_time": "2025-02-11T09:17:49.720717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['n'])\n",
    "def g_jit_decorated(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "x = g_jit_decorated(10, 20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0c7a17265aee4",
   "metadata": {},
   "source": [
    "The first time we call a JIT function it gets compiled and the resulting XLA code is cached.  Subsequent calls will then reuse the cached code. If we specify static arguments, the cached code will only be used for the same values of static arguments. To find the cached code JAX uses the hash of the function, this implies that we should not redefine equivalent functions in ways that can modify the function hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95747f53fcad795c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:49.745870Z",
     "start_time": "2025-02-11T09:17:49.743643Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unjitted_loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted_partial(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this! each time the partial returns\n",
    "    # a function with different hash\n",
    "    i = jax.jit(partial(unjitted_loop_body))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_lambda(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this!, lambda will also return\n",
    "    # a function with a different hash\n",
    "    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_normal(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # this is OK, since JAX can find the\n",
    "    # cached, compiled function\n",
    "    i = jax.jit(unjitted_loop_body)(i)\n",
    "  return x + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa5488d0a0897f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:17:57.896076Z",
     "start_time": "2025-02-11T09:17:49.754314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 ms Â± 9.75 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "220 ms Â± 6.08 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "565 Î¼s Â± 2.56 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
    "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n",
    "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acdf6a26d0fb70",
   "metadata": {},
   "source": [
    "## Automatic Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c99956d416a0e",
   "metadata": {},
   "source": [
    "Automatic vectorization allows us to extend a function defined on single input to support batch operations, while capitalizing on hardware acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31fee1dac025feed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:21:08.078557Z",
     "start_time": "2025-02-11T09:21:08.076477Z"
    }
   },
   "outputs": [],
   "source": [
    "def activation_score(x):\n",
    "    # x is a 1D array.\n",
    "    return jax.nn.tanh(jnp.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2707b5c4662a03ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:21:09.734975Z",
     "start_time": "2025-02-11T09:21:09.691942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation (single sample): 0.46211717\n"
     ]
    }
   ],
   "source": [
    "# Single input example (1D array)\n",
    "x_single = jnp.array([0.2, 0.5, 0.8])\n",
    "print(\"Activation (single sample):\", activation_score(x_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ac1cb42e5dd48c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:21:13.776739Z",
     "start_time": "2025-02-11T09:21:13.774515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batched inputs: each row represents one training example.\n",
    "x_batch = jnp.array([\n",
    "    [0.2, 0.5, 0.85],\n",
    "    [0.1, -0.3, 0.9],\n",
    "    [-0.4, 0.7, 0.6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffdc2139a7a3ea08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:21:15.664831Z",
     "start_time": "2025-02-11T09:21:15.662500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47512326\n",
      "0.22918896\n",
      "0.2913126\n"
     ]
    }
   ],
   "source": [
    "for lx in x_batch:\n",
    "    print(activation_score(lx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6119a620029ba1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:21:17.991789Z",
     "start_time": "2025-02-11T09:21:17.968297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct batch call activation:\n",
      " 0.33637553\n"
     ]
    }
   ],
   "source": [
    "# Direct call on the batch\n",
    "activation_direct = activation_score(x_batch)\n",
    "print(\"Direct batch call activation:\\n\", activation_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12bd74cc8efc9dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:22:17.115018Z",
     "start_time": "2025-02-11T09:22:17.110920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmap rows:\n",
      " [0.47512326 0.22918896 0.2913126 ]\n",
      "vmap cols:\n",
      " [-0.03332099  0.2913126   0.65461576]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the function over its first (and only) argument.\n",
    "vectorize_along_rows = jax.vmap(activation_score, in_axes=(0))\n",
    "vectorize_along_cols = jax.vmap(activation_score, in_axes=(1))\n",
    "\n",
    "# Now, applying it to the batch returns an activation per sample.\n",
    "activation_vmap = vectorize_along_rows(x_batch)\n",
    "print(\"vmap rows:\\n\", activation_vmap)\n",
    "\n",
    "print(\"vmap cols:\\n\", vectorize_along_cols(x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071b2ed09ae3e46",
   "metadata": {},
   "source": [
    "what about a more complex example (perhaps this should be the only one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6a569c574cd5e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:31:20.590948Z",
     "start_time": "2025-02-11T09:31:20.589025Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_array_sum(scale: float, weights: jnp.ndarray, epsilon: float):\n",
    "    return scale * jnp.sum(weights) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a332c84569c9230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:31:21.379893Z",
     "start_time": "2025-02-11T09:31:21.376584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3.0001, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_array_sum(0.5, jnp.array([1,2,3]), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388110175753bbb7",
   "metadata": {},
   "source": [
    "how can we extend this function to multiple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "713351c2cbf4cecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:38:37.824644Z",
     "start_time": "2025-02-11T09:38:37.821528Z"
    }
   },
   "outputs": [],
   "source": [
    "scales = jnp.array([0.5, 1.0, 1.5])  # Shape: (3,)\n",
    "weights = jnp.array([\n",
    "    [1.0, 0.2, 0.3],\n",
    "    [0.5, 0.8, 1.0],\n",
    "    [0.7, 0.1, 0.4]\n",
    "])\n",
    "epsilon = 1e-4\n",
    "\n",
    "# we can vectorize along the first two parameters. we'll interpret weights as a set of vectors and scales as a set of scalars.\n",
    "# (note that we do not say anything about epsilon because we don't vectorize over it)\n",
    "vectorized_weighted_array_sum = jax.vmap(weighted_array_sum, in_axes=(0, 0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c945a9ac25589853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:38:39.019799Z",
     "start_time": "2025-02-11T09:38:38.994927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.7501   , 2.3000998, 1.8001001], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_weighted_array_sum(scales, weights, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d96eb2b1e8784b",
   "metadata": {},
   "source": [
    "to achieve this without vmap we would have to use do a manual for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092690ab82a1cfd",
   "metadata": {},
   "source": [
    "the `in_axes` parameter specifies along which axes we vectorize for each of the inputs. in this case we don't have much room to play with but we could vectorize along the colums or the rows of the weight matrix.\n",
    "\n",
    "if our input data was higher dimensional we would have more freedom to play around with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726e8de5f14a2c0",
   "metadata": {},
   "source": [
    "there is also the `out_axes` parameter, which specifies how the results of batch computations are stored across the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f004f523e010c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:47:47.185792Z",
     "start_time": "2025-02-11T09:47:47.183407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1. , 0.2, 0.3],\n",
       "       [0.5, 0.8, 1. ],\n",
       "       [0.7, 0.1, 0.4]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "identity(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3d2e35bcf3f0b61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:50:05.354245Z",
     "start_time": "2025-02-11T09:50:05.351183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_rows(x) = \n",
      " [[1.  0.2 0.3]\n",
      " [0.5 0.8 1. ]\n",
      " [0.7 0.1 0.4]] \n",
      "\n",
      "id_cols(x) = \n",
      " [[1.  0.5 0.7]\n",
      " [0.2 0.8 0.1]\n",
      " [0.3 1.  0.4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"id_rows(x) = \\n\", jax.vmap(identity, in_axes=0, out_axes=0)(weights), \"\\n\")\n",
    "print(\"id_cols(x) = \\n\", jax.vmap(identity, in_axes=0, out_axes=1)(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab1865bcd04922",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8dfba6d8aa900",
   "metadata": {},
   "source": [
    "The automatic differentiation transform is a pretty simple transform that allows us to get the gradient of any function. We can apply it multiple times to obtain a higher order gradient. Also we can choose along which parameters to compute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "552872419e1c37f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:17:02.427646Z",
     "start_time": "2025-02-11T10:17:02.419367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t x: 4.0\n",
      "Gradient w.r.t y: 27.0\n",
      "Gradient both: (Array(4., dtype=float32, weak_type=True), Array(27., dtype=float32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "def func(x, y):\n",
    "    return x**2 + y**3\n",
    "\n",
    "grad_x = jax.grad(func, argnums=0)        # Derivative w.r.t. x\n",
    "grad_y = jax.grad(func, argnums=1)        # Derivative w.r.t. y\n",
    "grad   = jax.grad(func, argnums= (0, 1))  # Both\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "print(\"Gradient w.r.t x:\", grad_x(x, y))  # 2*x = 4\n",
    "print(\"Gradient w.r.t y:\", grad_y(x, y))  # 3*y^2 = 27\n",
    "print(\"Gradient both:\", grad(x, y))       # (4, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "386c6e924edb43a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T10:19:18.825592Z",
     "start_time": "2025-02-11T10:19:18.730500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: {'b': Array(30., dtype=float32, weak_type=True), 'w': Array(68., dtype=float32, weak_type=True)}\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(params, x):\n",
    "    w, b = params[\"w\"], params[\"b\"]\n",
    "    return jnp.sum((w * x + b) ** 2)\n",
    "\n",
    "params = {\"w\": 2.0, \"b\": 1.0}\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grads = grad_fn(params, x)\n",
    "print(\"Gradients:\", grads)  # {'w': ..., 'b': ...}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
